{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络数据处理与分析完整流程\n",
    "\n",
    "本 notebook 将 algorithms 文件夹中的各个步骤串联在一起，实现从原始专利数据到网络分析的完整流程。\n",
    "\n",
    "## 流程概述\n",
    "\n",
    "1. **数据预处理**\n",
    "   - 清洗专利数据\n",
    "   - 删除个人申请\n",
    "\n",
    "2. **网络构建**\n",
    "   - 单层网络构建\n",
    "   - 跨层耦合网络构建\n",
    "\n",
    "3. **网络层权重计算**\n",
    "\n",
    "4. **结构洞耦合分析和关键性指数计算**\n",
    "   - 结构洞耦合计算\n",
    "   - 结构洞耦合数据库构建\n",
    "   - 关键性指数计算\n",
    "\n",
    "5. **中心度耦合分析和核心性指数计算**\n",
    "   - 中心度耦合计算\n",
    "   - 中心度耦合数据库构建\n",
    "  - 核心性指数计算\n",
    "\n",
    "6. **关键性与核心性指标权重计算**\n",
    "   - 关键性-核心性数据库构建\n",
    "   - 指标权重计算\n",
    "\n",
    "7. **节点关键核心性计算**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置和导入"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 导入基础步骤的函数\n",
    "try:\n",
    "    from algorithms.step_1_clean_patent_data import clean_patent_data\n",
    "    from algorithms.step_1_remove_personal_application import remove_personal_applications\n",
    "\n",
    "# 导入知识、技术、合作研发网络构建函数\n",
    "    from algorithms.step_2_knowledge_network_construction import construct_knowledge_network\n",
    "    from algorithms.step_2_technology_network_construction import construct_technology_network\n",
    "    from algorithms.step_2_collaborative_RD_network_construction import construct_collaborative_RD_network\n",
    "    \n",
    "# 导入跨层网络构建函数\n",
    "    from algorithms.step_2_knowledge_technology_network_construction import construct_knowledge_technology_network\n",
    "    from algorithms.step_2_technology_collaborative_RD_network_construction import construct_technology_collaborative_RD_network\n",
    "    from algorithms.step_2_knowledge_collaborative_RD_network_construction import construct_knowledge_collaborative_RD_network\n",
    "    print(\"✓ 基础模块导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ 基础模块导入失败: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 导入分析步骤的函数\n",
    "try:\n",
    "    from algorithms.step_3_network_layer_weights import calculate_network_weights\n",
    "    from algorithms.step_4_structural_hole_coupling_calculation import calculate_structural_hole\n",
    "    from algorithms.step_4_structural_hole_coupling_database_construction import build_structural_hole_database\n",
    "    from algorithms.step_4_criticality_index_calculation import calculate_criticality\n",
    "    from algorithms.step_5_centrality_coupling_calculation import calculate_centrality_coupling\n",
    "    from algorithms.step_5_centrality_coupling_database_construction import build_centrality_coupling_database\n",
    "    from algorithms.step_5_centrality_index_calculation import calculate_centrality_index\n",
    "    from algorithms.step_6_criticality_and_centrality_database_construction import build_criticality_centrality_database\n",
    "    print(\"✓ 分析模块导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ 分析模块导入失败: {e}\")\n",
    "\n",
    "print(\"\\n所有模块导入完成！\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：数据预处理\n",
    "\n",
    "### 1.1 清洗专利数据\n",
    "将所有英文字母转化为小写字母（防止漏掉重复值）,只保留“公开（公告）号、引文专利公开号、施引专利公开号、IPC分类、专利权人”五列数据\n",
    "删除无IPC号和乱码数据\n",
    "删除“公开（公告）号”列的重复值"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# 设置路径（根据实际项目结构调整）\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'algorithms' else Path.cwd()\n",
    "\n",
    "# 输入输出路径配置\n",
    "input_file = PROJECT_ROOT / 'data' / 'input' / 'original_patent_data.xlsx'  # 默认Excel输入\n",
    "cleaned_output = PROJECT_ROOT / 'data' / 'step1_output' / 'patent_data_cleaned.csv'\n",
    "\n",
    "# 1. 自动处理输入文件\n",
    "if not input_file.exists():\n",
    "    # 尝试查找CSV版本\n",
    "    csv_input = input_file.with_suffix('.csv')\n",
    "    if csv_input.exists():\n",
    "        print(f\"使用CSV输入文件: {csv_input}\")\n",
    "        input_file = csv_input\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"输入文件不存在: {input_file} 或 {csv_input}\")\n",
    "else:\n",
    "    print(f\"使用Excel输入文件: {input_file}\")\n",
    "\n",
    "# 2. 确保输出目录存在\n",
    "cleaned_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. 执行数据清洗\n",
    "print(\"\\n=== 开始数据清洗 ===\")\n",
    "from algorithms.step_1_clean_patent_data import clean_patent_data\n",
    "\n",
    "try:\n",
    "    # 调用清洗函数（参数名与函数定义保持一致）\n",
    "    result = clean_patent_data(\n",
    "        input_path=str(input_file),    # 参数名对应函数定义的input_path\n",
    "        output_path=str(cleaned_output) # 参数名对应output_path\n",
    "    )\n",
    "    \n",
    "    # 打印处理结果\n",
    "    print(\"\\n清洗结果:\")\n",
    "    print(result)\n",
    "    \n",
    "    # 验证输出\n",
    "    if cleaned_output.exists():\n",
    "        df = pd.read_csv(cleaned_output)\n",
    "        print(f\"\\n输出验证: 共{len(df)}条记录\")\n",
    "        print(\"清洗后数据样例:\")\n",
    "        print(df.head(3))\n",
    "    else:\n",
    "        print(\"警告: 输出文件未生成\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n清洗失败: {str(e)}\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n✅ 数据清洗完成\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 删除个人申请专利"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 1.2: 去除个人申请 ===\")\n",
    "try:\n",
    "    result_1_2 = remove_personal_applications(input_path=\".\\data\\step1_output\\patent_data_cleaned.csv\", output_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\")\n",
    "    print(f\"结果: {result_1_2}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_1_2 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：网络构建\n",
    "\n",
    "### 2.1 单层网络构建"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 2.1: 知识网络构建 ===\")\n",
    "try:\n",
    "    result_2_1 = construct_knowledge_network(input_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\", output_dir=\".\\data\\step2_output\")\n",
    "    print(f\"结果: {result_2_1}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_2_1 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 2.2: 技术网络构建 ===\")\n",
    "try:\n",
    "    result_2_2 = construct_technology_network(input_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\", output_dir=\".\\data\\step2_output\")\n",
    "    print(f\"结果: {result_2_2}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_2_2 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 2.3: 协作研发网络构建 ===\")\n",
    "try:\n",
    "    result_2_3 = construct_collaborative_RD_network(input_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\", output_dir=\".\\data\\step2_output\")\n",
    "    print(f\"结果: {result_2_3}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_2_3 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 跨层耦合网络构建"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 2.4: 知识-技术耦合网络构建 ===\")\n",
    "try:\n",
    "    if construct_knowledge_technology_network:\n",
    "        result_2_4 = construct_knowledge_technology_network(input_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\", output_dir=\".\\data\\step2_output\")\n",
    "        print(f\"结果: {result_2_4}\")\n",
    "    else:\n",
    "        result_2_4 = \"函数导入失败\"\n",
    "        print(\"函数导入失败\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_2_4 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from algorithms.step_2_technology_collaborative_RD_network_construction import \\\n",
    "    construct_technology_collaborative_RD_network\n",
    "\n",
    "print(\"=== 步骤 2.5: 技术-协作研发耦合网络构建 ===\")\n",
    "try:\n",
    "    if construct_technology_collaborative_RD_network:\n",
    "        result_2_5 = construct_technology_collaborative_RD_network(input_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\", output_dir=\".\\data\\step2_output\")\n",
    "        print(f\"结果: {result_2_5}\")\n",
    "    else:\n",
    "        result_2_5 = \"函数导入失败\"\n",
    "        print(\"函数导入失败\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_2_5 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 2.6: 知识-协作研发耦合网络构建 ===\")\n",
    "try:\n",
    "    if construct_knowledge_collaborative_RD_network:\n",
    "        result_2_6 = construct_knowledge_collaborative_RD_network(input_path=\".\\data\\step1_output\\patent_data_selected_columns.csv\", output_dir=\".\\data\\step2_output\")\n",
    "        print(f\"结果: {result_2_6}\")\n",
    "    else:\n",
    "        result_2_6 = \"函数导入失败\"\n",
    "        print(\"函数导入失败\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_2_6 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：网络层权重计算"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== 步骤 3: 网络层权重计算 ===\")\n",
    "try:\n",
    "    result_3 = calculate_network_weights(input_dir=\".\\data\\step2_output\", output_dir=\".\\data\\step3_output\")\n",
    "    print(f\"结果: {result_3}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_3 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：结构洞耦合分析和关键性指数计算\n",
    "\n",
    "### 4.1 结构洞耦合计算"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"=== 步骤 4.1.1: 知识网络结构洞耦合计算 ===\")\n",
    "try:\n",
    "    # 使用Path对象处理路径\n",
    "    input_dir = Path(\"./data/step2_output\").resolve()\n",
    "    output_dir = Path(\"./data/step4_output\").resolve()\n",
    "    \n",
    "    # 正确参数名称为network_type\n",
    "    result_4_1_1 = calculate_structural_hole(\n",
    "        network_type=\"knowledge\",  # 修正参数名称\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    print(f\"结果: {result_4_1_1}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_4_1_1 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"=== 步骤 4.1.2: 技术网络结构洞耦合计算 ===\")\n",
    "try:\n",
    "    result_4_1_2 = calculate_structural_hole(\n",
    "        network_type=\"technology\",  # 修正参数名称\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    print(f\"结果: {result_4_1_2}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_4_1_2 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"=== 步骤 4.1.3: 合作研发网络结构洞耦合计算 ===\")\n",
    "try:\n",
    "    result_4_1_3 = calculate_structural_hole(\n",
    "        network_type=\"collaborative_R&D\",  # 注意保持连字符\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    print(f\"结果: {result_4_1_3}\")\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {e}\")\n",
    "    result_4_1_3 = f\"执行失败: {e}\"\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 结构洞耦合数据库构建"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"=== 步骤 4.2: 结构洞耦合数据库构建 ===\")\n",
    "try:\n",
    "    # 使用Path对象处理路径\n",
    "    step3_dir = Path(\"./data/step3_output\").resolve()\n",
    "    step4_dir = Path(\"./data/step4_output\").resolve()\n",
    "    \n",
    "    # 输入文件预验证\n",
    "    required_files = [\n",
    "        step3_dir / \"network_layer_weights.txt\",\n",
    "        step4_dir / \"knowledge_network_structural_hole_coupling.csv\",\n",
    "        step4_dir / \"technology_network_structural_hole_coupling.csv\",\n",
    "        step4_dir / \"collaborative_R&D_network_structural_hole_coupling.csv\"\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"缺失必要文件：\\n\" + \"\\n\".join([str(f) for f in missing_files]))\n",
    "\n",
    "    # 执行数据库构建\n",
    "    result_4_2 = build_structural_hole_database(\n",
    "        step3_dir=step3_dir,\n",
    "        step4_dir=step4_dir\n",
    "    )\n",
    "    print(f\"结果: {result_4_2}\")\n",
    "    \n",
    "    # 结果文件验证\n",
    "    output_file = step4_dir / \"structural_hole_coupling_database.csv\"\n",
    "    if output_file.exists():\n",
    "        df = pd.read_csv(output_file)\n",
    "        print(f\"\\n数据库预览：\")\n",
    "        print(df.head(3))\n",
    "        print(f\"\\n总计 {len(df)} 条记录\")\n",
    "    else:\n",
    "        print(\"警告：输出文件未生成\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {str(e)}\")\n",
    "    result_4_2 = f\"执行失败: {str(e)}\"\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 关键性指数计算"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 步骤 4.3: 多网络关键性指数计算 ===\")\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    # 解析绝对路径\n",
    "    step2_dir = Path(\"./data/step2_output\").resolve()\n",
    "    step4_dir = Path(\"./data/step4_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    # 必要输入文件列表\n",
    "    required_files = [\n",
    "        step4_dir / \"structural_hole_coupling_database.csv\",\n",
    "        step2_dir / \"knowledge_network_nodes.csv\",\n",
    "        step2_dir / \"technology_network_nodes.csv\",\n",
    "        step2_dir / \"collaborative_R&D_network_nodes.csv\",\n",
    "        step2_dir / \"knowledge-technology_network_edges.csv\",\n",
    "        step2_dir / \"knowledge-collaborative_R&D_network_edges.csv\",\n",
    "        step2_dir / \"technology-collaborative_R&D_network_edges.csv\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件缺失情况\n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"缺失必要文件:\\n\" + \"\\n\".join([f\"• {f.name}\" for f in missing_files]))\n",
    "    \n",
    "    # ==================== 执行计算 ====================\n",
    "    result_4_3 = calculate_criticality(\n",
    "        step2_dir=step2_dir,\n",
    "        step4_dir=step4_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 定义预期输出文件\n",
    "    output_files = [\n",
    "        \"knowledge_network_criticality_index.csv\",\n",
    "        \"technology_network_criticality_index.csv\",\n",
    "        \"collaborative_R&D_network_criticality_index.csv\"\n",
    "    ]\n",
    "    \n",
    "    # 检查输出文件并展示样例\n",
    "    for fname in output_files:\n",
    "        output_path = step4_dir / fname\n",
    "        if output_path.exists():\n",
    "            df = pd.read_csv(output_path)\n",
    "            print(f\"\\n✅ {fname}（前3行样例）:\")\n",
    "            print(df.head(3))\n",
    "            print(f\"记录总数: {len(df)}\")\n",
    "        else:\n",
    "            print(f\"\\n❌ {fname} 文件未生成\")\n",
    "\n",
    "    print(f\"\\n结果: {result_4_3}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {str(e)}\")\n",
    "    result_4_3 = f\"执行失败: {str(e)}\"\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：中心度耦合分析和核心指数计算\n",
    "\n",
    "### 5.1 中心度耦合计算"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 步骤 5.1: 中心度耦合指标计算 ===\")\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    # 解析绝对路径\n",
    "    input_dir = Path(\"./data/step2_output\").resolve()\n",
    "    output_dir = Path(\"./data/step5_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    # 必要输入文件列表\n",
    "    required_files = [\n",
    "        input_dir / \"knowledge_network_nodes.csv\",\n",
    "        input_dir / \"knowledge_network_edges.csv\",\n",
    "        input_dir / \"technology_network_nodes.csv\",\n",
    "        input_dir / \"technology_network_edges.csv\",\n",
    "        input_dir / \"collaborative_R&D_network_nodes.csv\",\n",
    "        input_dir / \"collaborative_R&D_network_edges.csv\"\n",
    "    ]\n",
    "    \n",
    "    # 检查文件缺失情况\n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"缺失必要文件:\\n\" + \"\\n\".join([f\"• {f.name}\" for f in missing_files])\n",
    "        )\n",
    "\n",
    "    # ==================== 执行计算 ====================\n",
    "    result_5_1 = calculate_centrality_coupling(\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 定义预期输出文件\n",
    "    output_files = [\n",
    "        \"knowledge_network_centrality_coupling.csv\",\n",
    "        \"technology_network_centrality_coupling.csv\",\n",
    "        \"collaborative_R&D_network_centrality_coupling.csv\"\n",
    "    ]\n",
    "    \n",
    "    # 检查输出文件并展示样例\n",
    "    for fname in output_files:\n",
    "        output_path = output_dir / fname\n",
    "        if output_path.exists():\n",
    "            df = pd.read_csv(output_path)\n",
    "            print(f\"\\n✅ {fname}（前3行样例）:\")\n",
    "            print(df.head(3))\n",
    "            print(f\"记录总数: {len(df)}\")\n",
    "            print(f\"中心度范围: {df['centrality_coupling'].min()} ~ {df['centrality_coupling'].max()}\")\n",
    "        else:\n",
    "            print(f\"\\n❌ {fname} 文件未生成\")\n",
    "\n",
    "    print(f\"\\n结果: {result_5_1}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {str(e)}\")\n",
    "    result_5_1 = f\"执行失败: {str(e)}\"\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 中心性耦合数据库构建"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 步骤 5.2: 中心度耦合数据库构建 ===\")\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    step3_dir = Path(\"./data/step3_output\").resolve()\n",
    "    step5_dir = Path(\"./data/step5_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    required_files = [\n",
    "        step3_dir / \"network_layer_weights.txt\",\n",
    "        step5_dir / \"knowledge_network_centrality_coupling.csv\",\n",
    "        step5_dir / \"technology_network_centrality_coupling.csv\",\n",
    "        step5_dir / \"collaborative_R&D_network_centrality_coupling.csv\"\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"缺失必要文件:\\n\" + \"\\n\".join([f\"• {f.name}\" for f in missing_files]))\n",
    "    \n",
    "    # ==================== 执行计算 ====================\n",
    "    result_5_2 = build_centrality_coupling_database(\n",
    "        step3_dir=step3_dir,\n",
    "        step5_dir=step5_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 验证主数据库文件\n",
    "    database_path = step5_dir / \"centrality_coupling_database.csv\"\n",
    "    if database_path.exists():\n",
    "        db_df = pd.read_csv(database_path)\n",
    "        print(f\"\\n✅ 中心度耦合数据库（前3行样例）:\")\n",
    "        print(db_df.head(3))\n",
    "        print(f\"总记录数: {len(db_df)}\")\n",
    "        \n",
    "        # 各网络记录统计\n",
    "        print(\"\\n各网络记录分布:\")\n",
    "        for layer_id, name in [(1,'知识网络'), (2,'技术网络'), (3,'合作研发网络')]:\n",
    "            count = len(db_df[db_df['网络层'] == layer_id])\n",
    "            print(f\"• {name}: {count} 条\")\n",
    "    else:\n",
    "        print(\"❌ 主数据库文件未生成\")\n",
    "\n",
    "    # 验证各网络输出文件\n",
    "    network_files = [\n",
    "        \"knowledge_network_centrality_coupling.csv\",\n",
    "        \"technology_network_centrality_coupling.csv\",\n",
    "        \"collaborative_R&D_network_centrality_coupling.csv\"\n",
    "    ]\n",
    "    for fname in network_files:\n",
    "        output_path = step5_dir / fname\n",
    "        if output_path.exists():\n",
    "            df = pd.read_csv(output_path)\n",
    "            print(f\"\\n✅ {fname}（前3行样例）:\")\n",
    "            print(df.head(3))\n",
    "        else:\n",
    "            print(f\"\\n❌ {fname} 文件未生成\")\n",
    "\n",
    "    print(f\"\\n结果: {result_5_2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"执行失败: {str(e)}\")\n",
    "    result_5_2 = f\"执行失败: {str(e)}\"\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 核心性指数计算"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 步骤 5.2: 核心性指数计算 ===\")\n",
    "\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    step2_dir = Path(\"./data/step2_output\").resolve()\n",
    "    step5_dir = Path(\"./data/step5_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    required_files = [\n",
    "        step2_dir / \"knowledge_network_nodes.csv\",\n",
    "        step2_dir / \"technology_network_nodes.csv\",\n",
    "        step2_dir / \"collaborative_R&D_network_nodes.csv\",\n",
    "        step2_dir / \"knowledge-technology_network_edges.csv\",\n",
    "        step2_dir / \"knowledge-collaborative_R&D_network_edges.csv\", \n",
    "        step2_dir / \"technology-collaborative_R&D_network_edges.csv\",\n",
    "        step5_dir / \"centrality_coupling_database.csv\"\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"缺失必要输入文件:\\n\" + \"\\n\".join([f\"• {f.name}\" for f in missing_files]))\n",
    "    \n",
    "    # ==================== 执行计算 ====================\n",
    "    from algorithms.step_5_centrality_index_calculation import calculate_centrality_index\n",
    "    result_5_3 = calculate_centrality_index(\n",
    "        step2_dir=step2_dir,\n",
    "        step5_dir=step5_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 检查各网络输出文件\n",
    "    output_files = [\n",
    "        \"knowledge_network_centrality_index.csv\",\n",
    "        \"technology_network_centrality_index.csv\",\n",
    "        \"collaborative_R&D_network_centrality_index.csv\"\n",
    "    ]\n",
    "    \n",
    "    for name in output_files:\n",
    "        output_path = step5_dir / fname\n",
    "        if output_path.exists():\n",
    "            df = pd.read_csv(output_path)\n",
    "            print(f\"\\n✅ {fname}（统计信息）:\")\n",
    "            print(f\"- 记录数: {len(df)}\")\n",
    "            print(f\"- 中心度指数范围: {df['centrality_index'].min():.4f} ~ {df['centrality_index'].max():.4f}\")\n",
    "            print(f\"- 平均值: {df['centrality_index'].mean():.4f}\")\n",
    "            print(\"前3行样例:\")\n",
    "            print(df.head(3))\n",
    "        else:\n",
    "            print(f\"\\n❌ {fname} 文件未生成\")\n",
    "    \n",
    "    print(f\"\\n处理结果: {result_5_2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 执行失败: {str(e)}\")\n",
    "    result_5_3 = f\"执行失败: {str(e)}\"\n",
    "    \n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：关键性与核心性指标权重计算"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.1 关键性-核心性数据库构建"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 步骤6.1: 关键性-核心性数据库构建 ===\")\n",
    "\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    step4_dir = Path(\"./data/step4_output\").resolve()\n",
    "    step5_dir = Path(\"./data/step5_output\").resolve()\n",
    "    step6_dir = Path(\"./data/step6_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    required_files = [\n",
    "        step4_dir / \"knowledge_network_criticality_index.csv\",\n",
    "        step4_dir / \"technology_network_criticality_index.csv\",\n",
    "        step4_dir / \"collaborative_R&D_network_criticality_index.csv\",\n",
    "        step5_dir / \"knowledge_network_centrality_index.csv\",\n",
    "        step5_dir / \"technology_network_centrality_index.csv\",\n",
    "        step5_dir / \"collaborative_R&D_network_centrality_index.csv\"\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"缺失必要输入文件:\\n\" + \"\\n\".join([f\"• {f.name}\" for f in missing_files]))\n",
    "    \n",
    "    # ==================== 执行计算 ====================\n",
    "    from algorithms.step_6_criticality_and_centrality_database_construction import build_criticality_centrality_database  # 替换为实际模块路径\n",
    "    result = build_criticality_centrality_database(\n",
    "        step4_dir=step4_dir,\n",
    "        step5_dir=step5_dir,\n",
    "        output_dir=step6_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 主数据库验证\n",
    "    db_path = step6_dir / \"criticality_and_centrality_database.csv\"\n",
    "    if db_path.exists():\n",
    "        db_df = pd.read_csv(db_path)\n",
    "        print(f\"\\n✅ 主数据库（关键统计指标）:\")\n",
    "        print(f\"- 总记录数: {len(db_df)}\")\n",
    "        print(f\"- 知识网络节点: {len(db_df[db_df['网络层'] == 1])}\")\n",
    "        print(f\"- 技术网络节点: {len(db_df[db_df['网络层'] == 2])}\")\n",
    "        print(f\"- 合作研发网络节点: {len(db_df[db_df['网络层'] == 3])}\")\n",
    "        print(f\"- 关键性均值: {db_df['关键性'].mean():.4f}\")\n",
    "        print(f\"- 核心性均值: {db_df['核心性'].mean():.4f}\")\n",
    "        print(\"\\n前3行样例:\")\n",
    "        print(db_df.head(3))\n",
    "    else:\n",
    "        print(\"\\n❌ 主数据库文件未生成\")\n",
    "    \n",
    "    # 网络层数据验证\n",
    "    print(\"\\n网络层数据完整性检查:\")\n",
    "    for layer, name in [(1,'知识网络'), (2,'技术网络'), (3,'合作研发网络')]:\n",
    "        layer_df = db_df[db_df['网络层'] == layer]\n",
    "        print(f\"• {name}: {len(layer_df)}条记录 | \"\n",
    "              f\"关键性[{layer_df['关键性'].min():.2f}-{layer_df['关键性'].max():.2f}] | \"\n",
    "              f\"核心性[{layer_df['核心性'].min():.2f}-{layer_df['核心性'].max():.2f}]\")\n",
    "    \n",
    "    print(f\"\\n处理结果: {result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 执行失败: {str(e)}\")\n",
    "    result = f\"执行失败: {str(e)}\"\n",
    "    \n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.2 指标权重计算"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== 步骤6.2: 指标权重计算 ===\")\n",
    "\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    step6_dir = Path(\"./data/step6_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    required_file = step6_dir / \"criticality_and_centrality_database.csv\"\n",
    "    if not required_file.exists():\n",
    "        raise FileNotFoundError(f\"❌ 缺失必要输入文件: {required_file.name}\")\n",
    "    \n",
    "    # ==================== 执行计算 ====================\n",
    "    from algorithms.step_6_index_weights import calculate_index_weights  # 替换为实际模块路径\n",
    "    result = calculate_index_weights(\n",
    "        input_dir=step6_dir,\n",
    "        output_dir=step6_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 权重文件验证\n",
    "    weights_file = step6_dir / \"index_weights.txt\"\n",
    "    if weights_file.exists():\n",
    "        print(f\"\\n✅ 权重计算结果:\")\n",
    "        with open(weights_file, 'r') as f:\n",
    "            for line in f:\n",
    "                print(f\"  • {line.strip()}\")\n",
    "        \n",
    "        # 验证权重逻辑一致性\n",
    "        weights = {}\n",
    "        with open(weights_file, 'r') as f:\n",
    "            for line in f:\n",
    "                key, value = line.strip().split(': ')\n",
    "                weights[key] = float(value)\n",
    "        \n",
    "        print(f\"\\n权重验证:\")\n",
    "        print(f\"  • 关键性权重: {weights['criticality_weight']:.4f}\")\n",
    "        print(f\"  • 核心性权重: {weights['centrality_weight']:.4f}\")\n",
    "        print(f\"  • 权重总和: {weights['criticality_weight'] + weights['centrality_weight']:.4f} (应≈1.0)\")\n",
    "        \n",
    "        # 加载原始数据验证计算逻辑\n",
    "        df = pd.read_csv(required_file)\n",
    "        print(f\"\\n原始数据统计:\")\n",
    "        print(f\"  • 关键性范围: {df['关键性'].min():.2f} - {df['关键性'].max():.2f}\")\n",
    "        print(f\"  • 核心性范围: {df['核心性'].min():.2f} - {df['核心性'].max():.2f}\")\n",
    "        print(f\"  • 关键性/核心性相关系数: {df['关键性'].corr(df['核心性']):.4f}\")\n",
    "    else:\n",
    "        print(\"\\n❌ 权重文件未生成\")\n",
    "    \n",
    "    print(f\"\\n处理结果: {result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 执行失败: {str(e)}\")\n",
    "    result = f\"执行失败: {str(e)}\"\n",
    "    \n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七步： 节点关键核心性计算"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 步骤7: 节点关键核心性计算 ===\")\n",
    "\n",
    "try:\n",
    "    # ==================== 路径配置 ====================\n",
    "    step6_dir = Path(\"./data/step6_output\").resolve()\n",
    "    step7_dir = Path(\"./data/step7_output\").resolve()\n",
    "    \n",
    "    # ==================== 输入验证 ====================\n",
    "    required_files = [\n",
    "        step6_dir / \"criticality_and_centrality_database.csv\",\n",
    "        step6_dir / \"index_weights.txt\"\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not f.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"缺失必要输入文件:\\n\" + \"\\n\".join([f\"• {f.name}\" for f in missing_files]))\n",
    "    \n",
    "    # ==================== 执行计算 ====================\n",
    "    from algorithms.step_7_criticality_centrality_index_calculation import calculate_critical_centrality_index  # 替换为实际模块路径\n",
    "    result = calculate_critical_centrality_index(\n",
    "        input_dir=step6_dir,\n",
    "        output_dir=step7_dir\n",
    "    )\n",
    "    \n",
    "    # ==================== 结果验证 ====================\n",
    "    print(\"\\n\" + \"-\"*40 + \" 结果验证 \" + \"-\"*40)\n",
    "    \n",
    "    # 主输出文件验证\n",
    "    output_file = step7_dir / \"criticality-centrality_index.csv\"\n",
    "    if output_file.exists():\n",
    "        df = pd.read_csv(output_file)\n",
    "        print(f\"\\n✅ 关键-核心性指数数据库:\")\n",
    "        print(f\"- 总记录数: {len(df)}\")\n",
    "        print(f\"- 关键性权重: {df['关键性权重'].iloc[0]:.4f}\")\n",
    "        print(f\"- 核心性权重: {df['核心性权重'].iloc[0]:.4f}\")\n",
    "        print(f\"- 关键核心性范围: {df['关键核心性'].min():.4f} ~ {df['关键核心性'].max():.4f}\")\n",
    "        print(f\"- 平均值: {df['关键核心性'].mean():.4f}\")\n",
    "        \n",
    "        # 各网络层统计\n",
    "        print(\"\\n分网络统计:\")\n",
    "        for layer, name in [(1,'知识网络'), (2,'技术网络'), (3,'合作研发网络')]:\n",
    "            layer_df = df[df['网络层'] == layer]\n",
    "            print(f\"  • {name}: {len(layer_df)}节点 | \"\n",
    "                  f\"关键核心性[{layer_df['关键核心性'].min():.4f}-{layer_df['关键核心性'].max():.4f}]\")\n",
    "        \n",
    "        print(\"\\n前3行样例:\")\n",
    "        print(df.head(3))\n",
    "    else:\n",
    "        print(\"\\n❌ 输出文件未生成\")\n",
    "    \n",
    "    print(f\"\\n处理结果: {result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 执行失败: {str(e)}\")\n",
    "    result = f\"执行失败: {str(e)}\"\n",
    "    \n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
